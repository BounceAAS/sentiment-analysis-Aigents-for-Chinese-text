{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50423cb-e815-4586-859b-f25f42001548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as seg\n",
    "from snownlp import SnowNLP\n",
    "import pynlpir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sentiment_analysis_with_jieba(text):\n",
    "    analysis_words = []\n",
    "    for i in seg.cut(text):\n",
    "        analysis_words.append((i.word, i.flag))\n",
    "    #print(analysis_words)\n",
    "\n",
    "    keywords = []\n",
    "    for i in analysis_words:\n",
    "        # Take out adj, adv and normal noun from the list\n",
    "        if i[1] in [\"a\", \"d\", \"v\"]:\n",
    "            keywords.append(i[0])\n",
    "    print(\"keywords=\", keywords)\n",
    "\n",
    "    postive_num = 0\n",
    "    negative_num = 0\n",
    "    sentiments_list = []\n",
    "    for i in keywords:\n",
    "        sl = SnowNLP(i)\n",
    "        if sl.sentiments > 0.6:\n",
    "            postive_num = postive_num + 1\n",
    "        elif sl.sentiments < 0.4:\n",
    "            negative_num = negative_num + 1\n",
    "        sentiments_list.append((i, sl.sentiments))\n",
    "\n",
    "    print(sentiments_list)\n",
    "    if (postive_num + negative_num) != 0:\n",
    "        sentiment = postive_num / (postive_num + negative_num)\n",
    "    else:\n",
    "        #could modify this criteria\n",
    "        sentiment = 0.5\n",
    "\n",
    "    if sentiment >= 0.5:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "    #return sentiment\n",
    "    \n",
    "def sentiment_analysis_without_jieba(text):\n",
    "    s = SnowNLP(text)\n",
    "    sentiment = s.sentiments\n",
    "    print(text, sentiment)\n",
    "\n",
    "    if sentiment >= 0.5:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "    #return sentiment\n",
    "\n",
    "def sentiment_analysis_for_distinct_word(text):\n",
    "    splited_text = [word for word in text]\n",
    "    postive_num = 0\n",
    "    negative_num = 0\n",
    "    sentiments_list = []\n",
    "    for i in splited_text:\n",
    "        sl = SnowNLP(i)\n",
    "        if sl.sentiments > 0.6:\n",
    "            postive_num = postive_num + 1\n",
    "        elif sl.sentiments < 0.4:\n",
    "            negative_num = negative_num + 1\n",
    "        sentiments_list.append((i, sl.sentiments))\n",
    "    \n",
    "    #print(sentiments_list)\n",
    "    if (postive_num + negative_num) != 0:\n",
    "        sentiment = postive_num / (postive_num + negative_num)\n",
    "    else:\n",
    "        #could modify this criteria\n",
    "        sentiment = 0.5\n",
    "        \n",
    "    if sentiment >= 0.5:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "    #return sentiment\n",
    "\n",
    "def sentiment_analysis_for_distinct_word_without_snownlp(text):\n",
    "    #load the lexicons\n",
    "    path_pos = \"lexicon_positive_chinese.txt\"\n",
    "    path_neg = \"lexicon_negative_chinese.txt\"\n",
    "    lexicon_pos, lexicon_neg  = read_lexicon(path_pos, path_neg)\n",
    "    \n",
    "    splited_text = [word for word in text]\n",
    "    postive_num = 0\n",
    "    negative_num = 0\n",
    "    \n",
    "    for i in splited_text:\n",
    "        if i in lexicon_pos:\n",
    "            postive_num = postive_num + 1\n",
    "        elif i in lexicon_neg:\n",
    "            negative_num = negative_num + 1\n",
    "            \n",
    "    if (postive_num + negative_num) != 0:\n",
    "        sentiment = postive_num / (postive_num + negative_num)\n",
    "    else:\n",
    "        #could modify this criteria\n",
    "        sentiment = 0.5\n",
    "\n",
    "    if sentiment >= 0.5:\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "    #return sentiment\n",
    "\n",
    "def build_ngrams(sentence_list, N):\n",
    "    if N < 2:\n",
    "        return sentence_list\n",
    "    size = len(sentence_list) - N + 1\n",
    "    if size < 1:\n",
    "        return None\n",
    "    items = []\n",
    "    for i in range(size):\n",
    "        items.append(\"\".join(sentence_list[i:i+N]))\n",
    "    return items\n",
    "\n",
    "def sentiment_with_ngrams(input_text, Max_N = 5):    \n",
    "    #count the number of positive and negative\n",
    "    pc=[]\n",
    "    nc=[]\n",
    "    # Parse the input text\n",
    "    splited_input_text = [word for word in input_text]\n",
    "    #seq = parse(input_text)\n",
    "    #print(splited_input_text)\n",
    "    punctuations = [\"，\", \"。\", \"、\", \"？\", \"：\", \"“\", \"”\", \" \"]\n",
    "    p, n = 0, 0\n",
    "    for N in range(Max_N,0,-1):  #do this reversely, first check large n-gram's sentiment\n",
    "        seq_ngrams = build_ngrams(splited_input_text, N)\n",
    "        print(seq_ngrams)\n",
    "        if seq_ngrams:\n",
    "            i = 0\n",
    "            while i < len(seq_ngrams):\n",
    "                w = seq_ngrams[i]\n",
    "                #print(w)\n",
    "                #check if it is a puntuation\n",
    "                has_punctuation = False\n",
    "                for j in punctuations:\n",
    "                    if j in w:\n",
    "                        has_punctuation = True\n",
    "                        continue\n",
    "                #check if it is full n-gram\n",
    "                if has_punctuation == True or w == \"\" or len(w) != N:\n",
    "                    i = i + 1\n",
    "                    continue\n",
    "                #analysis the sentiment of this n-gram\n",
    "                found = False\n",
    "                s = SnowNLP(w)\n",
    "                sentiment_snownlp = s.sentiments\n",
    "                #if it is a positive word(check by dictionary of snownlp)\n",
    "                if sentiment_snownlp > 0.6:\n",
    "                    pc.append(w)\n",
    "                    p = N + p\n",
    "                    found = True\n",
    "                #if it is a negative word(check by dictionary of snownlp)    \n",
    "                elif sentiment_snownlp < 0.4:\n",
    "                    nc.append(w)\n",
    "                    n = N + n\n",
    "                    found = True\n",
    "                #if found this n-gram has sentiment, then skip the following n-1 single words\n",
    "                #and set those single word in splited_input as none in case the program check it again in lower N\n",
    "                if found:\n",
    "                    for j in range(N):\n",
    "                        splited_input_text[i+j] = \"\"\n",
    "                    i = i + N\n",
    "                else:\n",
    "                    i = i + 1\n",
    "                print(\"pc=\",pc)\n",
    "                print(\"nc=\",nc)\n",
    "                #print(\"p=\",p)\n",
    "                #print(\"n=\",n)\n",
    "                #print(splited_input_text)\n",
    "    \n",
    "    if len(pc) >= len(nc):\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "\n",
    "def read_lexicon(path_pos, path_neg):\n",
    "    f_pos = open(path_pos, encoding=\"utf-8\")\n",
    "    f_neg = open(path_neg, encoding=\"utf-8\")\n",
    "    lexicon_pos = f_pos.readlines()\n",
    "    lexicon_pos = [line.strip(\"\\n\") for line in lexicon_pos]\n",
    "    lexicon_neg = f_neg.readlines()\n",
    "    lexicon_neg = [line.strip(\"\\n\") for line in lexicon_neg]\n",
    "    f_pos.close()\n",
    "    f_neg.close()\n",
    "    return lexicon_pos, lexicon_neg\n",
    "\n",
    "\n",
    "def sentiment_with_ngrams_without_snownlp(input_text, Max_N = 5):   \n",
    "    #load the lexicons\n",
    "    path_pos = \"lexicon_positive_chinese.txt\"\n",
    "    path_neg = \"lexicon_negative_chinese.txt\"\n",
    "    lexicon_pos, lexicon_neg  = read_lexicon(path_pos, path_neg)\n",
    "    \n",
    "    #count the number of positive and negative\n",
    "    pc=[]\n",
    "    nc=[]\n",
    "    # Parse the input text\n",
    "    splited_input_text = [word for word in input_text]\n",
    "    #seq = parse(input_text)\n",
    "    #print(splited_input_text)\n",
    "    punctuations = [\"，\", \"。\", \"、\", \"？\", \"：\", \"“\", \"”\", \" \"]\n",
    "    p, n = 0, 0\n",
    "    for N in range(Max_N,0,-1):  #do this reversely, first check large n-gram's sentiment\n",
    "        seq_ngrams = build_ngrams(splited_input_text, N)\n",
    "        print(seq_ngrams)\n",
    "        if seq_ngrams:\n",
    "            i = 0\n",
    "            while i < len(seq_ngrams):\n",
    "                w = seq_ngrams[i]\n",
    "                #print(w)\n",
    "                #check if it is a puntuation\n",
    "                has_punctuation = False\n",
    "                for j in punctuations:\n",
    "                    if j in w:\n",
    "                        has_punctuation = True\n",
    "                        continue\n",
    "                #check if it is full n-gram\n",
    "                if has_punctuation == True or w == \"\" or len(w) != N:\n",
    "                    i = i + 1\n",
    "                    continue\n",
    "                #analysis the sentiment of this n-gram\n",
    "                found = False\n",
    "                #if it is a positive word(check by dictionary of snownlp)\n",
    "                if w in lexicon_pos:\n",
    "                    pc.append(w)\n",
    "                    p = N + p\n",
    "                    found = True\n",
    "                #if it is a negative word(check by dictionary of snownlp)    \n",
    "                elif w in lexicon_neg:\n",
    "                    nc.append(w)\n",
    "                    n = N + n\n",
    "                    found = True\n",
    "                #if found this n-gram has sentiment, then skip the following n-1 single words\n",
    "                #and set those single word in splited_input as none in case the program check it again in lower N\n",
    "                if found:\n",
    "                    for j in range(N):\n",
    "                        splited_input_text[i+j] = \"\"\n",
    "                    i = i + N\n",
    "                else:\n",
    "                    i = i + 1\n",
    "                print(\"pc=\",pc)\n",
    "                print(\"nc=\",nc)\n",
    "                '''\n",
    "                print(\"p=\",p)\n",
    "                print(\"n=\",n)\n",
    "                print(splited_input_text)'''\n",
    "    \n",
    "    if len(pc) >= len(nc):\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "\n",
    "def sentiment_analysis_with_jieba_without_snownlp(text):\n",
    "    #load the lexicons\n",
    "    path_pos = \"lexicon_positive_chinese.txt\"\n",
    "    path_neg = \"lexicon_negative_chinese.txt\"\n",
    "    lexicon_pos, lexicon_neg  = read_lexicon(path_pos, path_neg)\n",
    "    \n",
    "    #count the number of positive and negative\n",
    "    pc=[]\n",
    "    nc=[]\n",
    "    # Parse the input text\n",
    "    \n",
    "    analysis_words = []\n",
    "    for i in seg.cut(text):\n",
    "        analysis_words.append((i.word, i.flag))\n",
    "    #print(analysis_words)\n",
    "\n",
    "    keywords = []\n",
    "    for i in analysis_words:\n",
    "        # Take out adj, adv and normal noun from the list\n",
    "        if i[1] in [\"a\", \"d\", \"v\"]:\n",
    "            keywords.append(i[0])\n",
    "    print(\"keywords=\", keywords)\n",
    "\n",
    "    p, n = 0, 0\n",
    "    \n",
    "    for w in keywords:\n",
    "        if w in lexicon_pos:\n",
    "            pc.append(w)\n",
    "            p = len(w) + p\n",
    "            found = True\n",
    "                #if it is a negative word(check by dictionary of snownlp)    \n",
    "        elif w in lexicon_neg:\n",
    "            nc.append(w)\n",
    "            n = len(w) + n\n",
    "            found = True\n",
    "\n",
    "    #print(sentiments_list)\n",
    "    if len(pc) >= len(nc):\n",
    "        return \"P\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "    #return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bdba08-aa3e-4725-826d-df96a746f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_csv= (6, 1)\n",
      "['Comments_in_chinese']\n",
      "keywords= []\n",
      "[]\n",
      "keywords= ['不坏']\n",
      "[('不坏', 0.8999999999999999)]\n",
      "keywords= ['不好']\n",
      "[('不好', 0.24509033778476041)]\n",
      "keywords= ['坏']\n",
      "[('坏', 0.33423913043478237)]\n",
      "keywords= []\n",
      "[]\n",
      "keywords= ['坏']\n",
      "[('坏', 0.33423913043478237)]\n",
      "shape_csv= (6, 2)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba\n",
      "0                 好消息                             P\n",
      "1               不坏的消息                             P\n",
      "2               不好的消息                             N\n",
      "3                坏的消息                             N\n",
      "4                只是消息                             P\n",
      "5               坏的好消息                             N\n",
      "好消息 0.4467309078976355\n",
      "不坏的消息 0.792230848408126\n",
      "不好的消息 0.12091746448919471\n",
      "坏的消息 0.1753936531064676\n",
      "只是消息 0.2574107392864008\n",
      "坏的好消息 0.26737349498904595\n",
      "shape_csv= (6, 3)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba  \\\n",
      "0                 好消息                             P   \n",
      "1               不坏的消息                             P   \n",
      "2               不好的消息                             N   \n",
      "3                坏的消息                             N   \n",
      "4                只是消息                             P   \n",
      "5               坏的好消息                             N   \n",
      "\n",
      "  sentiment_analysis_without_jieba  \n",
      "0                                N  \n",
      "1                                P  \n",
      "2                                N  \n",
      "3                                N  \n",
      "4                                N  \n",
      "5                                N  \n",
      "shape_csv= (6, 4)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba  \\\n",
      "0                 好消息                             P   \n",
      "1               不坏的消息                             P   \n",
      "2               不好的消息                             N   \n",
      "3                坏的消息                             N   \n",
      "4                只是消息                             P   \n",
      "5               坏的好消息                             N   \n",
      "\n",
      "  sentiment_analysis_without_jieba sentiment_analysis_distinct_word  \n",
      "0                                N                                P  \n",
      "1                                P                                N  \n",
      "2                                N                                N  \n",
      "3                                N                                N  \n",
      "4                                N                                N  \n",
      "5                                N                                N  \n",
      "shape_csv= (6, 5)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba  \\\n",
      "0                 好消息                             P   \n",
      "1               不坏的消息                             P   \n",
      "2               不好的消息                             N   \n",
      "3                坏的消息                             N   \n",
      "4                只是消息                             P   \n",
      "5               坏的好消息                             N   \n",
      "\n",
      "  sentiment_analysis_without_jieba sentiment_analysis_distinct_word  \\\n",
      "0                                N                                P   \n",
      "1                                P                                N   \n",
      "2                                N                                N   \n",
      "3                                N                                N   \n",
      "4                                N                                N   \n",
      "5                                N                                N   \n",
      "\n",
      "  sentiment_analysis_for_distinct_word_without_snownlp  \n",
      "0                                                  P    \n",
      "1                                                  N    \n",
      "2                                                  N    \n",
      "3                                                  P    \n",
      "4                                                  P    \n",
      "5                                                  P    \n",
      "None\n",
      "None\n",
      "['好消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['好消', '消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= ['消息']\n",
      "['好', '', '']\n",
      "pc= ['好']\n",
      "nc= ['消息']\n",
      "['不坏的消息']\n",
      "pc= ['不坏的消息']\n",
      "nc= []\n",
      "['', '']\n",
      "['', '', '']\n",
      "['', '', '', '']\n",
      "['', '', '', '', '']\n",
      "['不好的消息']\n",
      "pc= []\n",
      "nc= ['不好的消息']\n",
      "['', '']\n",
      "['', '', '']\n",
      "['', '', '', '']\n",
      "['', '', '', '', '']\n",
      "None\n",
      "['坏的消息']\n",
      "pc= []\n",
      "nc= ['坏的消息']\n",
      "['', '']\n",
      "['', '', '']\n",
      "['', '', '', '']\n",
      "None\n",
      "['只是消息']\n",
      "pc= []\n",
      "nc= ['只是消息']\n",
      "['', '']\n",
      "['', '', '']\n",
      "['', '', '', '']\n",
      "['坏的好消息']\n",
      "pc= []\n",
      "nc= ['坏的好消息']\n",
      "['', '']\n",
      "['', '', '']\n",
      "['', '', '', '']\n",
      "['', '', '', '', '']\n",
      "shape_csv= (6, 6)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba  \\\n",
      "0                 好消息                             P   \n",
      "1               不坏的消息                             P   \n",
      "2               不好的消息                             N   \n",
      "3                坏的消息                             N   \n",
      "4                只是消息                             P   \n",
      "5               坏的好消息                             N   \n",
      "\n",
      "  sentiment_analysis_without_jieba sentiment_analysis_distinct_word  \\\n",
      "0                                N                                P   \n",
      "1                                P                                N   \n",
      "2                                N                                N   \n",
      "3                                N                                N   \n",
      "4                                N                                N   \n",
      "5                                N                                N   \n",
      "\n",
      "  sentiment_analysis_for_distinct_word_without_snownlp  \\\n",
      "0                                                  P     \n",
      "1                                                  N     \n",
      "2                                                  N     \n",
      "3                                                  P     \n",
      "4                                                  P     \n",
      "5                                                  P     \n",
      "\n",
      "  sentiment_analysis_with_ngrams  \n",
      "0                              P  \n",
      "1                              P  \n",
      "2                              N  \n",
      "3                              N  \n",
      "4                              N  \n",
      "5                              N  \n",
      "None\n",
      "None\n",
      "['好消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['好消', '消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['好', '消', '息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['不坏的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['不坏的消', '坏的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['不坏的', '坏的消', '的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['不坏', '坏的', '的消', '消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "['不', '', '', '消', '息']\n",
      "pc= []\n",
      "nc= ['坏的', '不']\n",
      "pc= []\n",
      "nc= ['坏的', '不']\n",
      "pc= []\n",
      "nc= ['坏的', '不']\n",
      "['不好的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['不好的消', '好的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['不好的', '好的消', '的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['不好', '好的', '的消', '消息']\n",
      "pc= []\n",
      "nc= ['不好']\n",
      "pc= []\n",
      "nc= ['不好']\n",
      "pc= []\n",
      "nc= ['不好']\n",
      "['', '', '的', '消', '息']\n",
      "pc= []\n",
      "nc= ['不好']\n",
      "pc= []\n",
      "nc= ['不好']\n",
      "pc= []\n",
      "nc= ['不好']\n",
      "None\n",
      "['坏的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['坏的消', '的消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['坏的', '的消', '消息']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "['', '', '消', '息']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "None\n",
      "['只是消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['只是消', '是消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['只是', '是消', '消息']\n",
      "pc= ['只是']\n",
      "nc= []\n",
      "pc= ['只是']\n",
      "nc= []\n",
      "['', '', '消', '息']\n",
      "pc= ['只是']\n",
      "nc= []\n",
      "pc= ['只是']\n",
      "nc= []\n",
      "['坏的好消息']\n",
      "pc= []\n",
      "nc= []\n",
      "['坏的好消', '的好消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['坏的好', '的好消', '好消息']\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "pc= []\n",
      "nc= []\n",
      "['坏的', '的好', '好消', '消息']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "['', '', '好', '消', '息']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "pc= []\n",
      "nc= ['坏的']\n",
      "shape_csv= (6, 7)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba  \\\n",
      "0                 好消息                             P   \n",
      "1               不坏的消息                             P   \n",
      "2               不好的消息                             N   \n",
      "3                坏的消息                             N   \n",
      "4                只是消息                             P   \n",
      "5               坏的好消息                             N   \n",
      "\n",
      "  sentiment_analysis_without_jieba sentiment_analysis_distinct_word  \\\n",
      "0                                N                                P   \n",
      "1                                P                                N   \n",
      "2                                N                                N   \n",
      "3                                N                                N   \n",
      "4                                N                                N   \n",
      "5                                N                                N   \n",
      "\n",
      "  sentiment_analysis_for_distinct_word_without_snownlp  \\\n",
      "0                                                  P     \n",
      "1                                                  N     \n",
      "2                                                  N     \n",
      "3                                                  P     \n",
      "4                                                  P     \n",
      "5                                                  P     \n",
      "\n",
      "  sentiment_analysis_with_ngrams sentiment_with_ngrams_without_snownlp  \n",
      "0                              P                                     P  \n",
      "1                              P                                     N  \n",
      "2                              N                                     N  \n",
      "3                              N                                     N  \n",
      "4                              N                                     P  \n",
      "5                              N                                     N  \n",
      "keywords= []\n",
      "keywords= ['不坏']\n",
      "keywords= ['不好']\n",
      "keywords= ['坏']\n",
      "keywords= []\n",
      "keywords= ['坏']\n",
      "shape_csv= (6, 8)\n",
      "  Comments_in_chinese sentiment_analysis_with_jieba  \\\n",
      "0                 好消息                             P   \n",
      "1               不坏的消息                             P   \n",
      "2               不好的消息                             N   \n",
      "3                坏的消息                             N   \n",
      "4                只是消息                             P   \n",
      "5               坏的好消息                             N   \n",
      "\n",
      "  sentiment_analysis_without_jieba sentiment_analysis_distinct_word  \\\n",
      "0                                N                                P   \n",
      "1                                P                                N   \n",
      "2                                N                                N   \n",
      "3                                N                                N   \n",
      "4                                N                                N   \n",
      "5                                N                                N   \n",
      "\n",
      "  sentiment_analysis_for_distinct_word_without_snownlp  \\\n",
      "0                                                  P     \n",
      "1                                                  N     \n",
      "2                                                  N     \n",
      "3                                                  P     \n",
      "4                                                  P     \n",
      "5                                                  P     \n",
      "\n",
      "  sentiment_analysis_with_ngrams sentiment_with_ngrams_without_snownlp  \\\n",
      "0                              P                                     P   \n",
      "1                              P                                     N   \n",
      "2                              N                                     N   \n",
      "3                              N                                     N   \n",
      "4                              N                                     P   \n",
      "5                              N                                     N   \n",
      "\n",
      "  sentiment_analysis_with_jieba_without_snownlp  \n",
      "0                                             P  \n",
      "1                                             P  \n",
      "2                                             N  \n",
      "3                                             P  \n",
      "4                                             P  \n",
      "5                                             P  \n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_csv(\"comments.csv\")\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(list(comments))\n",
    "\n",
    "comments[\"sentiment_analysis_with_jieba\"] = np.nan\n",
    "comments[\"sentiment_analysis_with_jieba\"] = comments[\"Comments_in_chinese\"].apply(sentiment_analysis_with_jieba)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "comments[\"sentiment_analysis_without_jieba\"] = np.nan\n",
    "comments[\"sentiment_analysis_without_jieba\"] = comments[\"Comments_in_chinese\"].apply(sentiment_analysis_without_jieba)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "comments[\"sentiment_analysis_distinct_word\"] = np.nan\n",
    "comments[\"sentiment_analysis_distinct_word\"] = comments[\"Comments_in_chinese\"].apply(sentiment_analysis_for_distinct_word)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "comments[\"sentiment_analysis_for_distinct_word_without_snownlp\"] = np.nan\n",
    "comments[\"sentiment_analysis_for_distinct_word_without_snownlp\"] = comments[\"Comments_in_chinese\"].apply(sentiment_analysis_for_distinct_word_without_snownlp)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "\n",
    "comments[\"sentiment_analysis_with_ngrams\"] = np.nan\n",
    "comments[\"sentiment_analysis_with_ngrams\"] = comments[\"Comments_in_chinese\"].apply(sentiment_with_ngrams)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "comments[\"sentiment_with_ngrams_without_snownlp\"] = np.nan\n",
    "comments[\"sentiment_with_ngrams_without_snownlp\"] = comments[\"Comments_in_chinese\"].apply(sentiment_with_ngrams_without_snownlp)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "comments[\"sentiment_analysis_with_jieba_without_snownlp\"] = np.nan\n",
    "comments[\"sentiment_analysis_with_jieba_without_snownlp\"] = comments[\"Comments_in_chinese\"].apply(sentiment_analysis_with_jieba_without_snownlp)\n",
    "print(\"shape_csv=\", comments.shape)\n",
    "print(comments.head(10))\n",
    "\n",
    "comments.to_excel(\"Comment_analysised.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c507ec-d882-44c6-aaca-5c2e6be1a68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295cafe4-f584-4740-b2e7-d7d044e286a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
